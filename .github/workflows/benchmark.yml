name: Performance Benchmark

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: Single File Performance Benchmark
    runs-on: macos-latest
    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Swift 6.1
        uses: swift-actions/setup-swift@v2
        with:
          swift-version: "6.1"

      - name: Build package
        run: swift build

      - name: Run Single File Benchmark
        id: benchmark
        run: |
          echo "ðŸš€ Running single file benchmark..."
          # Run benchmark with ES2004a file and save results to JSON
          swift run fluidaudio benchmark --auto-download --single-file ES2004a --output benchmark_results.json

          # Extract key metrics from JSON output
          if [ -f benchmark_results.json ]; then
            # Parse JSON results (using basic tools available in GitHub runners)
            AVERAGE_DER=$(cat benchmark_results.json | grep -o '"averageDER":[0-9]*\.?[0-9]*' | cut -d':' -f2)
            AVERAGE_JER=$(cat benchmark_results.json | grep -o '"averageJER":[0-9]*\.?[0-9]*' | cut -d':' -f2) 
            PROCESSED_FILES=$(cat benchmark_results.json | grep -o '"processedFiles":[0-9]*' | cut -d':' -f2)
            
            # Get first result details
            RTF=$(cat benchmark_results.json | grep -o '"realTimeFactor":[0-9]*\.?[0-9]*' | head -1 | cut -d':' -f2)
            DURATION=$(cat benchmark_results.json | grep -o '"durationSeconds":[0-9]*\.?[0-9]*' | head -1 | cut -d':' -f2)
            SPEAKER_COUNT=$(cat benchmark_results.json | grep -o '"speakerCount":[0-9]*' | head -1 | cut -d':' -f2)
            
            echo "DER=${AVERAGE_DER}" >> $GITHUB_OUTPUT
            echo "JER=${AVERAGE_JER}" >> $GITHUB_OUTPUT  
            echo "RTF=${RTF}" >> $GITHUB_OUTPUT
            echo "DURATION=${DURATION}" >> $GITHUB_OUTPUT
            echo "SPEAKER_COUNT=${SPEAKER_COUNT}" >> $GITHUB_OUTPUT
            echo "PROCESSED_FILES=${PROCESSED_FILES}" >> $GITHUB_OUTPUT
            echo "SUCCESS=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Benchmark failed - no results file generated"
            echo "SUCCESS=false" >> $GITHUB_OUTPUT
          fi
        timeout-minutes: 25

      - name: Comment PR with Benchmark Results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const success = '${{ steps.benchmark.outputs.SUCCESS }}' === 'true';

            let comment = '## ðŸŽ¯ Single File Benchmark Results\n\n';

            if (success) {
              const der = parseFloat('${{ steps.benchmark.outputs.DER }}').toFixed(1);
              const jer = parseFloat('${{ steps.benchmark.outputs.JER }}').toFixed(1);
              const rtf = parseFloat('${{ steps.benchmark.outputs.RTF }}').toFixed(2);
              const duration = parseFloat('${{ steps.benchmark.outputs.DURATION }}').toFixed(1);
              const speakerCount = '${{ steps.benchmark.outputs.SPEAKER_COUNT }}';
              
              comment += `**Test File:** ES2004a (${duration}s audio)\n\n`;
              comment += '| Metric | Value | Target | Status |\n';
              comment += '|--------|-------|--------|---------|\n';
              comment += `| **DER** (Diarization Error Rate) | ${der}% | < 30% | ${der < 30 ? 'âœ…' : 'âŒ'} |\n`;
              comment += `| **JER** (Jaccard Error Rate) | ${jer}% | < 25% | ${jer < 25 ? 'âœ…' : 'âŒ'} |\n`;
              comment += `| **RTF** (Real-Time Factor) | ${rtf}x | < 1.0x | ${rtf < 1.0 ? 'âœ…' : 'âŒ'} |\n`;
              comment += `| **Speakers Detected** | ${speakerCount} | - | â„¹ï¸ |\n\n`;
              
              // Performance assessment
              if (der < 20) {
                comment += 'ðŸŽ‰ **Excellent Performance!** - Competitive with state-of-the-art research\n';
              } else if (der < 30) {
                comment += 'âœ… **Good Performance** - Meeting target benchmarks\n';
              } else {
                comment += 'âš ï¸ **Performance Below Target** - Consider parameter optimization\n';
              }
              
              comment += '\nðŸ“Š **Research Comparison:**\n';
              comment += '- Powerset BCE (2023): 18.5% DER\n';
              comment += '- EEND (2019): 25.3% DER\n';
              comment += '- x-vector clustering: 28.7% DER\n';
              
            } else {
              comment += 'âŒ **Benchmark Failed**\n\n';
              comment += 'The single file benchmark could not complete successfully. ';
              comment += 'This may be due to:\n';
              comment += '- Network issues downloading test data\n';
              comment += '- Model initialization problems\n';
              comment += '- Audio processing errors\n\n';
              comment += 'Please check the workflow logs for detailed error information.';
            }

            comment += '\n\n---\n*Automated benchmark using AMI corpus ES2004a test file*';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

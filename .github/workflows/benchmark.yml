name: Performance Benchmark

on:
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: Single File Performance Benchmark
    runs-on: macos-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Swift 6.1
        uses: swift-actions/setup-swift@v2
        with:
          swift-version: "6.1"

      - name: Debug permissions
        run: |
          echo "üîç Debugging GitHub Actions permissions..."
          echo "Event: ${{ github.event_name }}"
          echo "Actor: ${{ github.actor }}"
          echo "Repository: ${{ github.repository }}"
          echo "Head ref: ${{ github.head_ref }}"
          echo "Base ref: ${{ github.base_ref }}"
          echo "Token permissions debug:"
          gh auth status || echo "No gh auth available"
          echo "PR details:"
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "  PR number: ${{ github.event.pull_request.number }}"
            echo "  PR from fork: ${{ github.event.pull_request.head.repo.fork }}"
            echo "  PR author: ${{ github.event.pull_request.user.login }}"
          fi

      - name: Build package
        run: swift build

      - name: Download AMI annotations
        run: |
          echo "üì• Downloading AMI annotations for CI..."
          swift run fluidaudio download --dataset ami-annotations

      - name: Run Single File Benchmark
        id: benchmark
        env:
          DER_THRESHOLD: "20.0"
          JER_THRESHOLD: "23.0"
          RTF_THRESHOLD: "0.1"
        run: |
          echo "Running single file benchmark with strict thresholds..."
          echo "   DER Threshold: ${DER_THRESHOLD}%"
          echo "   JER Threshold: ${JER_THRESHOLD}%"
          echo "   RTF Threshold: ${RTF_THRESHOLD}x"
          echo ""
          echo "Environment info:"
          uname -a
          swift --version
          echo "Hardware info:"
          sysctl -a | grep machdep.cpu || true
          echo ""

          # Run benchmark with strict thresholds
          # Job will fail if any threshold is exceeded
          swift run fluidaudio benchmark \
            --auto-download \
            --single-file ES2004a \
            --output benchmark_results.json \
            --der-threshold ${DER_THRESHOLD} \
            --jer-threshold ${JER_THRESHOLD} \
            --rtf-threshold ${RTF_THRESHOLD}

          # If we reach here, the benchmark passed all thresholds
          echo "SUCCESS=true" >> $GITHUB_OUTPUT
        timeout-minutes: 25

      - name: Show benchmark_results.json
        if: always()
        run: |
          echo "--- benchmark_results.json ---"
          cat benchmark_results.json || echo "benchmark_results.json not found"
          echo "-----------------------------"

      - name: Extract benchmark metrics with jq
        id: extract
        run: |
          DER=$(jq '.averageDER' benchmark_results.json)
          JER=$(jq '.averageJER' benchmark_results.json)
          RTF=$(jq '.results[0].realTimeFactor' benchmark_results.json)
          DURATION=$(jq '.results[0].durationSeconds' benchmark_results.json)
          SPEAKER_COUNT=$(jq '.results[0].speakerCount' benchmark_results.json)
          GROUND_TRUTH_SPEAKER_COUNT=$(jq '.results[0].groundTruthSpeakerCount' benchmark_results.json)

          # Extract detailed timing information
          TOTAL_TIME=$(jq '.results[0].timings.totalProcessingSeconds' benchmark_results.json)
          MODEL_DOWNLOAD_TIME=$(jq '.results[0].timings.modelDownloadSeconds' benchmark_results.json)
          MODEL_COMPILE_TIME=$(jq '.results[0].timings.modelCompilationSeconds' benchmark_results.json)
          AUDIO_LOAD_TIME=$(jq '.results[0].timings.audioLoadingSeconds' benchmark_results.json)
          SEGMENTATION_TIME=$(jq '.results[0].timings.segmentationSeconds' benchmark_results.json)
          EMBEDDING_TIME=$(jq '.results[0].timings.embeddingExtractionSeconds' benchmark_results.json)
          CLUSTERING_TIME=$(jq '.results[0].timings.speakerClusteringSeconds' benchmark_results.json)
          INFERENCE_TIME=$(jq '.results[0].timings.totalInferenceSeconds' benchmark_results.json)

          echo "DER=${DER}" >> $GITHUB_OUTPUT
          echo "JER=${JER}" >> $GITHUB_OUTPUT
          echo "RTF=${RTF}" >> $GITHUB_OUTPUT
          echo "DURATION=${DURATION}" >> $GITHUB_OUTPUT
          echo "SPEAKER_COUNT=${SPEAKER_COUNT}" >> $GITHUB_OUTPUT
          echo "GROUND_TRUTH_SPEAKER_COUNT=${GROUND_TRUTH_SPEAKER_COUNT}" >> $GITHUB_OUTPUT
          echo "TOTAL_TIME=${TOTAL_TIME}" >> $GITHUB_OUTPUT
          echo "MODEL_DOWNLOAD_TIME=${MODEL_DOWNLOAD_TIME}" >> $GITHUB_OUTPUT
          echo "MODEL_COMPILE_TIME=${MODEL_COMPILE_TIME}" >> $GITHUB_OUTPUT
          echo "AUDIO_LOAD_TIME=${AUDIO_LOAD_TIME}" >> $GITHUB_OUTPUT
          echo "SEGMENTATION_TIME=${SEGMENTATION_TIME}" >> $GITHUB_OUTPUT
          echo "EMBEDDING_TIME=${EMBEDDING_TIME}" >> $GITHUB_OUTPUT
          echo "CLUSTERING_TIME=${CLUSTERING_TIME}" >> $GITHUB_OUTPUT
          echo "INFERENCE_TIME=${INFERENCE_TIME}" >> $GITHUB_OUTPUT

      - name: Update PR with Benchmark Results
        if: always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const der = parseFloat('${{ steps.extract.outputs.DER }}');
            const jer = parseFloat('${{ steps.extract.outputs.JER }}');
            const rtf = parseFloat('${{ steps.extract.outputs.RTF }}');
            const duration = parseFloat('${{ steps.extract.outputs.DURATION }}').toFixed(1);
            const speakerCount = '${{ steps.extract.outputs.SPEAKER_COUNT }}';
            const groundTruthSpeakerCount = '${{ steps.extract.outputs.GROUND_TRUTH_SPEAKER_COUNT }}';
            const totalTime = parseFloat('${{ steps.extract.outputs.TOTAL_TIME }}');
            const inferenceTime = parseFloat('${{ steps.extract.outputs.INFERENCE_TIME }}');
            const modelDownloadTime = parseFloat('${{ steps.extract.outputs.MODEL_DOWNLOAD_TIME }}');
            const modelCompileTime = parseFloat('${{ steps.extract.outputs.MODEL_COMPILE_TIME }}');
            const audioLoadTime = parseFloat('${{ steps.extract.outputs.AUDIO_LOAD_TIME }}');
            const segmentationTime = parseFloat('${{ steps.extract.outputs.SEGMENTATION_TIME }}');
            const embeddingTime = parseFloat('${{ steps.extract.outputs.EMBEDDING_TIME }}');
            const clusteringTime = parseFloat('${{ steps.extract.outputs.CLUSTERING_TIME }}');

            // Get job outcome from workflow
            const jobOutcome = '${{ job.status }}';
            const derThreshold = parseFloat('${{ env.DER_THRESHOLD }}');
            const jerThreshold = parseFloat('${{ env.JER_THRESHOLD }}');
            const rtfThreshold = parseFloat('${{ env.RTF_THRESHOLD }}');

            // Build the comment body
            let comment = '## Single File Benchmark Results\n\n';
            comment += `**Test File:** ES2004a (${duration}s audio)\n`;
            comment += `**Overall Result:** ${jobOutcome === 'success' ? '‚úÖ' : '‚ùå'}\n\n`;
            comment += '### Accuracy Metrics\n';
            comment += '| Metric | Value |\n';
            comment += '|--------|-------|\n';
            comment += `| **DER** (Diarization Error Rate) | ${der.toFixed(1)}% |\n`;
            comment += `| **JER** (Jaccard Error Rate) | ${jer.toFixed(1)}% |\n`;
            comment += `| **RTF** (Real-Time Factor) | ${rtf.toFixed(2)}x |\n`;
            comment += `| **Speakers Detected** | ${speakerCount}/${groundTruthSpeakerCount} |\n\n`;

            comment += '### ‚è±Ô∏è Performance Timing\n';
            comment += '| Stage | Time (s) | % of Total |\n';
            comment += '|-------|----------|------------|\n';
            comment += `| Model Download | ${modelDownloadTime.toFixed(3)} | ${(modelDownloadTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| Model Compilation | ${modelCompileTime.toFixed(3)} | ${(modelCompileTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| Audio Loading | ${audioLoadTime.toFixed(3)} | ${(audioLoadTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| Segmentation | ${segmentationTime.toFixed(3)} | ${(segmentationTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| Embedding Extraction | ${embeddingTime.toFixed(3)} | ${(embeddingTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| Speaker Clustering | ${clusteringTime.toFixed(3)} | ${(clusteringTime/totalTime*100).toFixed(1)}% |\n`;
            comment += `| **Total Processing** | **${totalTime.toFixed(3)}** | **100%** |\n\n`;

            comment += `**Inference Time**: ${inferenceTime.toFixed(3)}s (${(inferenceTime/totalTime*100).toFixed(1)}% of total)\n`;
            comment += `**Setup Overhead**: ${(modelDownloadTime + modelCompileTime).toFixed(3)}s (${((modelDownloadTime + modelCompileTime)/totalTime*100).toFixed(1)}% of total)\n\n`;

            // Performance assessment based on actual job outcome
            if (jobOutcome !== 'success') {
              comment += 'One or more thresholds exceeded\n';
              comment += `DER: ${der.toFixed(1)}% ‚ùå\n`;
              comment += `JER: ${jer.toFixed(1)}% ‚ùå\n`;
              comment += `RTF: ${rtf.toFixed(2)}x ‚ùå\n`;
            }

            comment += '\nResearch Comparison:\n';
            comment += '- Powerset BCE (2023): 18.5% DER\n';
            comment += '- EEND (2019): 25.3% DER\n';
            comment += '- x-vector clustering: 28.7% DER\n';

            try {
              // First, try to find existing benchmark comment
              const comments = await github.rest.issues.listComments({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
              });

              // Look for existing benchmark comment (identified by the header)
              const existingComment = comments.data.find(comment =>
                comment.body.includes('## Single File Benchmark Results') &&
                comment.user.type === 'Bot'
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  comment_id: existingComment.id,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
                console.log('‚úÖ Successfully updated existing benchmark comment');
              } else {
                // Create new comment if none exists
                await github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
                console.log('‚úÖ Successfully posted new benchmark results comment');
              }
            } catch (error) {
              console.error('‚ùå Failed to update/post comment:', error.message);
              if (error.status === 403) {
                console.error('üí° This is likely a permissions issue. Common causes:');
                console.error('   - PR from fork without proper permissions');
                console.error('   - GitHub token lacks issues:write permission');
                console.error('   - Repository settings restrict comment posting');
              }
              // Re-throw to let continue-on-error handle it
              throw error;
            }
